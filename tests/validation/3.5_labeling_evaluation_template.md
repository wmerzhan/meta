# Three-Color Labeling Evaluation Framework

**Purpose:** Evaluate the effectiveness and accuracy of MAestro's ðŸŸ¢/ðŸŸ¡/ðŸ”´ data quality labels

---

## 1. Distribution Analysis

**Template:** `3.5_labeling_distribution_[tester_id].csv`

Track label distribution across all extracted papers per tester:

| Paper ID | Total Fields Extracted | ðŸŸ¢ (Green Count) | ðŸŸ¡ (Yellow Count) | ðŸ”´ (Red Count) | ðŸŸ¢ % | ðŸŸ¡ % | ðŸ”´ % | Notes |
|----------|----------------------|------------------|------------------|----------------|------|------|------|-------|
| Paper-001 | 8 | 7 | 1 | 0 | 87.5% | 12.5% | 0% | Straightforward |
| Paper-002 | 8 | 5 | 2 | 1 | 62.5% | 25% | 12.5% | Complex design |
| Paper-003 | 8 | 8 | 0 | 0 | 100% | 0% | 0% | Simple RCT |

**Aggregation by Tester:**

| Tester ID | Total Fields | Green Count | Yellow Count | Red Count | Green % | Yellow % | Red % | Avg Papers |
|-----------|-------------|------------|-------------|-----------|---------|---------|-------|------------|
| Tester-01 | 112 | 72 | 28 | 12 | 64.3% | 25.0% | 10.7% | 14 papers |
| Tester-02 | 96 | 62 | 24 | 10 | 64.6% | 25.0% | 10.4% | 12 papers |

**Expected Distribution (from Epic 1-2 testing):**
- ðŸŸ¢ Green: 60-70%
- ðŸŸ¡ Yellow: 20-30%
- ðŸ”´ Red: 5-10%

**Analysis Questions:**
- Do actual percentages align with expected? If significantly different, why?
- If testers show >70% green: Is Microscope overconfident? Are testers missing uncertainty?
- If testers show <60% green: Is Microscope overly conservative? Are testers drowning in review burden?

---

## 2. False Positive Rate Calculation

**Definition:** Data labeled ðŸ”´ (Red/Failed) that is actually CORRECT according to gold standard

**Evaluation Process:**

For each ðŸ”´ labeled data point:

```
TEMPLATE: False Positive Audit

Paper ID: ___________
Tester ID: ___________
Field: ___________

ðŸ”´ Label Reason (from Microscope): ___________
Example: "Effect size missing from analysis" or "Hallucinated value detected"

Gold Standard Value: ___________
(From published meta-analysis, tester manual extraction, etc.)

MAestro Extracted Value: ___________

Auditor Review:
  Is the gold standard correct?  [ ] Yes [ ] No [ ] Uncertain
  Is MAestro's extraction actually wrong? [ ] Yes [ ] No [ ] Uncertain

Verdict:
  [ ] TRUE POSITIVE (MAestro correctly flagged error; ðŸ”´ is justified)
  [ ] FALSE POSITIVE (MAestro flagged error, but data is actually correct; ðŸ”´ was wrong)
  [ ] UNCERTAIN (ambiguous; reasonable disagreement between sources)

Explanation: ___________

Auditor Notes: ___________
```

**Aggregation Template:**

| Paper ID | Field | MAestro Value | Gold Standard Value | MAestro Label | Verdict | TP/FP |
|----------|-------|---------------|--------------------|----------------|---------|-------|
| Smith-2015 | Sample Size | 480 | 480 | ðŸ”´ | FP (actually correct) | FP |
| Jones-2018 | Effect Size | OR=1.45 | OR=1.8 | ðŸ”´ | TP (correctly flagged error) | TP |
| Brown-2016 | Outcome | Depression (PHQ-9) | Mood symptoms | ðŸ”´ | UNCERTAIN (ambiguous) | UNCERTAIN |

**Calculation:**

```
False Positive Rate = (Count of False Positives) / (Total ðŸ”´ Labels) Ã— 100%

Example:
  Total ðŸ”´ labels across all testers: 50
  False Positives (data actually correct): 8
  False Positive Rate = 8/50 = 16%

Target: â‰¤20% (meaning â‰¥80% of red flags are legitimate issues)
```

**Interpretation:**
- **FP Rate â‰¤20%:** Microscope's red flags are reliable; testers can trust them
- **FP Rate 21-35%:** Microscope flags some false alarms; testers need to spot-check red data
- **FP Rate >35%:** Microscope flags too many false alarms; labeling system needs improvement

---

## 3. False Negative Rate Calculation

**Definition:** Data labeled ðŸŸ¢ (Green/High Confidence) that is actually INCORRECT

**Evaluation Process:**

During accuracy validation (Task 9), when comparing MAestro extractions to gold standards:

```
TEMPLATE: False Negative Audit

Paper ID: ___________
Tester ID: ___________
Field: ___________

ðŸŸ¢ Label (MAestro Confidence): Green / High confidence

MAestro Extracted Value: ___________

Gold Standard Value: ___________
(From published source, tester manual extraction, etc.)

MISMATCH DETECTED: Does MAestro value match gold standard?
  [ ] Yes, match (not a negative)
  [ ] No, mismatch (potential false negative)

If mismatch, auditor assesses:

Was this mismatch flagged appropriately?
  [ ] Labeled ðŸŸ¡ or ðŸ”´ (caught correctly)
  [ ] Labeled ðŸŸ¢ (MISSED - this is a FALSE NEGATIVE)

Severity of Error:
  [ ] Minor (rounding difference, <5%)
  [ ] Moderate (significant but recoverable)
  [ ] Major (fundamentally wrong value)

Root Cause:
  [ ] Hallucination (MAestro invented data)
  [ ] Missed extraction (data in paper but not extracted)
  [ ] Misinterpretation (data misread)
  [ ] Other: ___________

Auditor Notes: ___________
```

**Aggregation Template:**

| Paper ID | Field | MAestro Label | MAestro Value | Gold Standard Value | Match | Was Labeled Appropriately | FN/Correct |
|----------|-------|---------------|-------|-------------------|-------|---------------------------|-----------|
| Smith-2015 | Outcome | ðŸŸ¢ | Depression | Mood symptoms | No | No (should be ðŸŸ¡) | FN |
| Jones-2018 | Effect Size | ðŸŸ¢ | OR=1.5 | OR=1.5 | Yes | Yes (correct) | Correct |
| Brown-2016 | N per arm | ðŸŸ¢ | 100 | 110 | No | No (should be ðŸ”´) | FN |

**Calculation:**

```
False Negative Rate = (Count of False Negatives) / (Total Mismatches with Gold Standard) Ã— 100%

Example:
  Total accuracy validations across all testers: 60 field comparisons
  Mismatches found: 8
  False Negatives (mismatches labeled ðŸŸ¢): 2
  False Negative Rate = 2/8 = 25%

Target: â‰¤30% (meaning â‰¥70% of errors are flagged with ðŸŸ¡/ðŸ”´)
```

**Interpretation:**
- **FN Rate â‰¤30%:** Microscope catches most errors; occasional misses acceptable
- **FN Rate 31-50%:** Microscope misses many errors; testers must be cautious with green data
- **FN Rate >50%:** Microscope frequently misses errors; green labels unreliable

---

## 4. User Trust & Reliance Assessment

**Survey Questions (End-of-Beta):**

```
TRUST ASSESSMENT

1. Did you trust the three-color labeling (ðŸŸ¢/ðŸŸ¡/ðŸ”´) system?
   [ ] Strongly distrust - labels seemed random
   [ ] Somewhat distrust - labels didn't guide me well
   [ ] Neutral - labels were okay but I didn't rely on them
   [ ] Somewhat trust - labels generally guided my review appropriately
   [ ] Strongly trust - labels were reliable guides

   Explanation: ___________________________

2. How did you use the color labels in your workflow?
   [ ] Ignored them - extracted without paying attention
   [ ] Spot-checked green data - trusted labels but verified key fields
   [ ] Reviewed all yellow/red - treated them as guidance only
   [ ] Fully relied on labels - skipped review of green data
   [ ] Mixed approach: ___________

3. Did you spot-check data labeled ðŸŸ¢ (green/high confidence)?
   [ ] Never - I trusted green labels completely
   [ ] Rarely - only for critical fields like effect size
   [ ] Sometimes - about 50% of papers
   [ ] Often - I checked most green data
   [ ] Always - I verified every field regardless of label

4. When you found an error in your extraction, was it:
   [ ] Labeled ðŸ”´ (correctly flagged)
   [ ] Labeled ðŸŸ¡ (flagged but uncertain)
   [ ] Labeled ðŸŸ¢ (labeled as confident - FALSE NEGATIVE)
   [ ] I didn't find any errors

5. Confidence in green data accuracy:
   [ ] Very low - green data is just AI guess
   [ ] Low - would need expert review before publication
   [ ] Moderate - usable for exploration
   [ ] High - could use for preliminary analysis
   [ ] Very high - publication-ready with minimal review

6. Suggestions for improving the labeling system:
   ___________________________
```

**Analysis Template:**

| Question | Response Option | Count | % of Testers | Interpretation |
|----------|-----------------|-------|-------------|-----------------|
| Trust in labels | Strongly trust | 3 | 30% | ~1/3 of testers highly confident |
| Trust in labels | Somewhat trust | 4 | 40% | Majority has moderate confidence |
| Trust in labels | Neutral | 2 | 20% | Skeptical group |
| Spot-check behavior | Never | 2 | 20% | Over-reliant, may accept errors |
| Spot-check behavior | Rarely | 3 | 30% | Reasonable balance |
| Spot-check behavior | Often | 4 | 40% | Appropriately cautious |
| Spot-check behavior | Always | 1 | 10% | Very cautious, may over-review |

**Findings:**
- If >70% "often" or "always" spot-check: Labels may be working (users appropriately skeptical)
- If >70% "never" spot-check: Testers over-relying on labels (risky)
- If >50% report trust issues: Labeling system needs improvement

---

## 5. Integrated Analysis: Distribution + FP + FN

**Summary Table:**

```
LABELING EFFECTIVENESS SUMMARY

Metric | Target | Actual | Status | Implication
-------|--------|--------|--------|------------------
ðŸŸ¢ Distribution | 60-70% | __ % | âœ“/âœ— | Overconfident? Underconfident?
ðŸŸ¡ Distribution | 20-30% | __ % | âœ“/âœ— | Appropriate flagging?
ðŸ”´ Distribution | 5-10% | __ % | âœ“/âœ— | Error detection rate
False Positive Rate | â‰¤20% | __ % | âœ“/âœ— | Red flags reliable?
False Negative Rate | â‰¤30% | __ % | âœ“/âœ— | Errors caught?
User Trust (% trusting) | >60% | __ % | âœ“/âœ— | Users confident?
User Spot-check (% often/always) | >50% | __ % | âœ“/âœ— | Appropriate skepticism?
```

**Interpretation Matrix:**

| If ðŸŸ¢% >75 | AND FP% High | THEN: Microscope overconfident; recommend tighter thresholds |
| If ðŸŸ¢% <55 | AND FP% Low | THEN: Microscope overly conservative; testers frustrated |
| If FN% >40 | THEN: Critical issue; errors not caught; must improve accuracy |
| If User Trust <40% | THEN: Labeling system failing; redesign needed |
| If Spot-check >70% but FN% <20% | THEN: System working well; users appropriately cautious |

---

## 6. Recommendations for Labeling Improvement

Based on analysis, document recommendations:

```
LABELING IMPROVEMENT RECOMMENDATIONS

1. Threshold Adjustment
   Current: Distribution shows X% green, FP rate Y%
   Recommendation: Adjust Microscope confidence threshold to:
   [ ] Increase sensitivity (catch more errors, more yellow/red)
   [ ] Decrease sensitivity (reduce false alarms, more green)
   [ ] Keep current (acceptable balance)

2. User Guidance
   Current: Z% of users lack trust in labels
   Recommendation:
   [ ] Clarify documentation (users misunderstand meaning)
   [ ] Adjust labeling criteria (labels not matching user expectations)
   [ ] Add confidence score (e.g., ðŸŸ¢=95% confidence, ðŸŸ¡=60% confidence)

3. Prompt Refinement
   Current: FN rate A%, FP rate B%
   Recommendation:
   [ ] Retrain Microscope to catch more errors (reduce FN)
   [ ] Reduce false alarms (reduce FP)
   [ ] Focus on specific field types (e.g., effect size detection)

4. Workflow Integration
   Current: User spot-check rate C%
   Recommendation:
   [ ] Make labels more prominent (users not noticing)
   [ ] Add automation (auto-flag for review based on label + confidence)
   [ ] Provide per-field guidance (help users understand label meaning)
```

---

## Data Files Created

- `3.5_labeling_distribution_[tester_id].csv` â€” Per-tester label counts
- `3.5_labeling_false_positive_audit.csv` â€” Red flag accuracy
- `3.5_labeling_false_negative_audit.csv` â€” Error detection gaps
- `3.5_labeling_trust_survey_responses.md` â€” Tester feedback on labels
