# Beta Tester Expectations Document

## Overview

This document sets clear expectations about what MAestro is, what we're asking of you, and what success looks like.

---

## Part 1: What to Expect from MAestro

### The Honest Truth: This is MVP (Minimum Viable Product)

**MAestro is not production-grade software.** It works and it's useful, but it has rough edges. This is intentional. By testing the MVP, you help us refine it into a robust tool.

**What that means:**
- ‚úÖ Core functionality works (data extraction, compilation, basic analysis)
- ‚ö†Ô∏è Some edge cases will break
- ‚ö†Ô∏è Error messages might not be clear
- ‚ö†Ô∏è Performance might be slow on very large papers (50+ pages)
- ‚ö†Ô∏è UI/UX is minimal (function over form)

**What it doesn't mean:**
- ‚ùå The tool is unsafe (it's not)
- ‚ùå Your data is at risk (it stays on your machine)
- ‚ùå You need technical expertise to use it
- ‚ùå It will eat your research data

---

### Known Limitations (AC6)

#### 1. Document Size & Context Windows

**Limitation:** Microscope uses Claude AI with a context window. Very large papers may hit the limit.

**Impact:** Papers >20 pages might be processed in chunks or fail silently.

**Workaround:** Split very large papers manually or use multiple shorter extractions.

**Improvement timeline:** Story 3.6 (refinement) will improve chunking strategy.

---

#### 2. Table Extraction Accuracy

**Limitation:** Extracting tables from PDFs is hard. Complex tables (merged cells, nested headers) sometimes fail.

**Impact:** Tables with non-standard layouts might extract as plaintext or with alignment issues.

**Workaround:** Manual review of extracted tables; manual entry for complex cases.

**Improvement timeline:** Story 3.6 will improve table extraction with specialized models.

---

#### 3. Statistical Notation Inconsistency

**Limitation:** Different journals use different notation for effect sizes, confidence intervals, etc.

**Impact:** Extracted values might be in slightly different formats than expected.

**Workaround:** Standardization in the Compiler phase catches most issues.

**Improvement timeline:** Story 3.6 will add explicit notation standardization step.

---

#### 4. Hallucination Risk (AI-Generated Data)

**Limitation:** AI models sometimes "hallucinate"‚Äîgenerate plausible-sounding but incorrect data.

**Impact:** Extracted data might not exactly match the original paper on ~2-5% of papers.

**Mitigation:** Three-color labeling system flags confidence levels. Spot-check üî¥ (low-confidence) values.

**Workaround:** Manual verification of critical fields (effect sizes, sample sizes).

**Improvement timeline:** Story 3.6 will improve accuracy through better prompting and multi-pass validation.

---

#### 5. Unsupported File Formats

**Limitation:** Microscope works with PDF files. Scanned images or Word documents require conversion first.

**Impact:** Non-PDF papers won't work directly.

**Workaround:** Convert to PDF (free tools available), then proceed.

**Improvement timeline:** Story 3.6+ will add support for image-based PDFs and other formats.

---

#### 6. Missing Metadata Extraction

**Limitation:** Microscope doesn't extract: publication date, ISSN, DOI links, author affiliations, funding information.

**Impact:** Your compiled data will have core effect size data but not all metadata.

**Workaround:** Manually add metadata if needed; most analyses don't require it.

**Improvement timeline:** Story 3.6 will add optional metadata extraction.

---

#### 7. Limited Discipline Specialization

**Limitation:** Microscope uses generic prompts. Different disciplines have different conventions.

**Impact:** Psychology studies might extract differently than medical studies (different stat reporting norms).

**Workaround:** Manual adjustment of key fields; flexibility in data interpretation.

**Improvement timeline:** Story 3.6+ will add discipline-specific prompt templates.

---

### Expected Improvement Timeline

**Story 3.6 (Refinement, 2-4 weeks):**
- Improved table extraction
- Better notation standardization
- Discipline-specific tweaks
- Enhanced error messages

**Story 3.7+ (Future):**
- Metadata extraction
- Image-based PDF support
- Multi-pass validation
- Advanced statistical checks

**Post-MVP:**
- Publication-grade accuracy (>95%)
- Comprehensive metadata
- Team collaboration features

---

## Part 2: What We're Asking of You

### Time Commitment

**Total:** 10-20 hours over 4-6 weeks

**Breakdown:**
- Week 1: 2 hours (setup + first paper)
- Weeks 2-5: 2-4 hours per week (extract papers + feedback)
- Week 6: 2 hours (final survey + wrap-up)

**What counts toward your hours:**
- ‚úÖ Extracting papers (5 min/paper average)
- ‚úÖ Weekly check-in (30 min/week)
- ‚úÖ Spot-checking accuracy (30 min for 3-5 papers)
- ‚úÖ End-of-beta survey (20 min)
- ‚úÖ Bug reporting (varies; 5-15 min per bug)
- ‚ùå Doesn't count: Optional office hours, casual chat in Discord

**Realistic schedule:**
- **Very busy week:** 1-2 hours (extract 1-2 papers, skip office hours)
- **Normal week:** 3-4 hours (extract 4-6 papers, attend sync call)
- **Flexible week:** 5+ hours (extra exploration, help peers)

---

### Your Core Tasks

#### Task 1: Extract 10-20 Papers

**What:** Run Microscope to extract data from your research papers

**How:**
```
cd your_project/
maestro extract paper_1.pdf
maestro extract paper_2.pdf
# ... repeat for 10-20 papers
```

**Expected time:** ~5 minutes per paper (faster after first 2-3)

**Success criteria:** All papers run without blocking errors (minor issues are fine)

---

#### Task 2: Provide Weekly Feedback

**What:** Every Friday, answer 5 quick questions about your experience

**How:** Reply to Friday check-in post on GitHub Discussions or Discord

**Questions:**
1. Papers extracted this week: ___
2. What worked well: _______________
3. What frustrated you: _______________
4. What features are missing: _______________
5. Overall rating (1-5): ___

**Expected time:** 5-10 minutes per week

**Success criteria:** Respond to all 6 weekly check-ins (4-6 weeks)

---

#### Task 3: Spot-Check Accuracy

**What:** For 3-5 of your papers, manually verify extracted data matches originals

**How:**
1. Pick 3-5 papers randomly from your extraction batch
2. For each paper:
   - Open original PDF
   - Find the key data points: effect size, sample size, authors
   - Compare to extracted CSV
   - Count matches
3. Calculate accuracy: (matches / total fields) √ó 100%
4. Report in end-of-beta survey

**Expected time:** ~15 minutes per paper, ~1 hour total (one-time at end)

**Success criteria:** Provide accuracy % for at least 3 papers

---

#### Task 4: Test Edge Cases

**What:** Try extracting papers that are "unusual"‚Äîoutside your comfort zone

**How:** If you normally extract medical RCTs, try:
- A non-English-language paper (if bilingual)
- A very short paper (2-3 pages)
- A very large paper (30+ pages)
- A paper with unusual table layouts
- A paper with incomplete reporting

**Expected time:** 2-3 additional papers, ~15 min each

**Success criteria:** Report any breakage or unusual behavior

---

#### Task 5: Report Bugs Thoroughly

**When you find something broken:**

**Report template:**
```
Title: [Brief description]

What I was doing:
[Which phase? Extracting which paper? With what settings?]

What happened:
[Error message? Unexpected output? Crash?]

What I expected:
[What should have happened]

Severity:
[ ] Critical‚Äîcompletely blocked
[ ] Major‚Äîworkaround exists
[ ] Minor‚Äîcosmetic or edge case
```

**Expected time:** 5-15 minutes per bug (only when you find one)

**Success criteria:** Report any bugs you encounter; be specific about context

---

#### Task 6: Share Testimonials

**At the end of beta testing:**

**What we'll ask:**
- "Would you use MAestro for a real project?"
- "What was most valuable about it?"
- "What needs to improve?"
- "Can we quote you?" (with permission)

**Expected time:** 10 minutes (end-of-beta survey)

**Success criteria:** Provide honest feedback; optionally, allow us to use 1-2 quotes publicly

---

### What You're NOT Expected to Do

‚ùå **Don't:**
- Fix bugs (that's our job)
- Write documentation
- Lead meetings
- Commit to specific timeframes (life happens!)
- Provide secret/sensitive research data in public channels
- Test publication-grade accuracy (MVP stage)
- Debug system issues (installer problems, etc.)

---

## Part 3: Success Looks Like

### Success Criteria for You (as a Tester)

**By the end of beta testing, you will have:**

‚úÖ **1. Extracted data from 10-20 papers without blocking errors**
- "Blocking error" = can't proceed without manual intervention
- Minor issues (e.g., one table extracted wrong) = fine
- You should have working extractions for all papers

‚úÖ **2. Identified 1-2 improvements or new failure modes**
- You found something that doesn't work
- You reported it clearly
- Examples: "Table extraction fails on merged headers", "Handles OR effect sizes but not HR"

‚úÖ **3. Understand when to use MAestro vs. traditional tools**
- You know MAestro's strengths: exploratory work, large literature reviews, time savings
- You know its limitations: needs verification, not publication-grade, rough edges
- You can advise a colleague on whether to use it for their project

‚úÖ **4. Would recommend MAestro to a colleague (with caveats)**
- Not necessarily "production-ready"
- But you see the potential: "Once they fix X and Y, this will be amazing"
- You'd say: "Worth trying for exploratory work; not for publication-grade work yet"

---

### Success Criteria for MAestro (What We're Validating)

**We're looking for:**

üìä **Extraction Accuracy ‚â•90% on spot-checked papers**
- Your manual verification shows AI extraction matches originals 90%+ of the time
- Pass: 8+ of your 10 spot-checked papers show ‚â•90% accuracy

üìä **Time Savings ‚â•50% vs. your manual approach**
- Papers extracted 5 min/paper on average
- Your manual baseline: 10-15 min/paper
- Pass: ‚â•50% reduction confirmed across your papers

üêõ **Identified 10-20 failure modes across all testers**
- Combined across all 5-10 testers, we find at least 10-20 distinct issues
- These inform Story 3.6 (refinement)

üé§ **Gathered 5+ usable testimonials**
- 3+ public quotes from testers saying something positive
- Directly shapes marketing and roadmap

---

### Success Criteria for Us (Founder)

**By end of beta testing, we have:**

‚úÖ 5-10 beta testers successfully onboarded and participating

‚úÖ Data on accuracy, time savings, failure modes from real-world usage

‚úÖ Documented feedback on:
- What works well
- What needs fixing
- What new features are most wanted

‚úÖ Confirmed MVP is stable enough for early adopters

‚úÖ Roadmap for Story 3.6 (refinement) informed by tester feedback

---

## Part 4: Communication & Support Expectations

### How We'll Communicate

**Weekly (Every Friday):**
- Check-in survey posted
- You reply by Monday

**Bi-weekly (Mondays or Thursdays):**
- Optional office hours (30-60 min open call)
- Ask questions, report bugs, brainstorm

**As-needed:**
- Bug reports: Respond within 24-48 hours
- Blocking issues: Respond within 4-6 hours
- Feature requests: Collect and summarize weekly

### How You Can Reach Us

**GitHub Discussions:** Best for documentation-able issues
- Response time: 24-48 hours

**Discord:** Best for quick questions
- Response time: typically <12 hours on weekdays

**Email (direct):** For private/sensitive issues
- Response time: 24-48 hours

---

## Part 5: NDA & Data Handling

### Your Data Privacy

**Your research data:**
- Stays on your machine
- Never shared with us
- Never used for training
- You own your extracted CSV

**Your feedback:**
- May be shared publicly (anonymously or with attribution, your choice)
- May be used in documentation
- May be cited in academic papers about tool evaluation
- **Always ask first if real names/affiliations used**

### Confidentiality

**Do you have sensitive research?**
- Let us know. We can discuss NDA if needed
- Confidentiality arrangements available on request

**Share your research data:**
- Please don't post research PDFs/data in public Discord/Discussions
- Use private messages instead
- We protect your privacy

---

## Part 6: After Beta Testing Ends

### What Happens Next?

**Week 7 (Beta closes):**
- No more new testing
- Final survey due
- Stories 3.5-3.6 begin based on your feedback

**Weeks 8-12 (Refinement):**
- We address top 5-10 bugs you reported
- We implement most-requested features
- We improve accuracy and performance

**Week 12+ (Next Release):**
- Beta testers get first access to refined version
- Your feedback acknowledges you in release notes
- MAestro is ready for broader adoption

### Ongoing Community

**After beta, you:**
- Can continue using MAestro with your own Claude account
- Have access to community (Discord/Discussions)
- Get updates on improvements and new features
- Can participate in future testing (optional)

### Credit & Acknowledgment

**In release notes:** "Thanks to our beta testers: [names or anonymous]"

**In documentation:** Case studies featuring your use cases (if you opt in)

**In academic publications:** Acknowledgment of beta testing contributions

---

## Part 7: Real Talk

### What Could Go Wrong?

**1. You run into bugs**
- *What happens:* You report it; we prioritize it
- *You feel:* Frustrated, but we'll help troubleshoot
- *Timeline:* Major bugs fixed in Story 3.6 (2-4 weeks)

**2. Extraction is less accurate than you hoped**
- *What happens:* You spot-check and find 70% accuracy instead of 90%
- *You feel:* Disappointed
- *Context:* We learn from this; it informs accuracy improvements
- *Our commitment:* Transparent about what we found; no false promises in future

**3. Time savings aren't as good as hoped**
- *What happens:* Takes 8 min/paper instead of 5 min
- *You feel:* Skeptical of value prop
- *Context:* We learn which pieces slow things down
- *Our commitment:* Address bottlenecks in Story 3.6

**4. Communication is slow or unclear**
- *What happens:* You post a question; it takes 48 hours to get answer
- *You feel:* Unsupported
- *What to do:* Reach out directly. We'll adjust response time expectations
- *Our commitment:* Transparent about bandwidth; help where we can

### What's Within Your Control

‚úÖ **You can:**
- Provide specific, detailed feedback
- Report bugs with context
- Try edge cases and unusual papers
- Engage with the community
- Give honest assessments

‚úÖ **You control:**
- How many hours you commit (flexible!)
- Which papers you extract
- Whether you share publicly
- When you participate (async-friendly)

---

## Part 8: Quick Reference Checklist

**Before you start:**
- [ ] Read this document (10 min)
- [ ] Read Quick Start Guide (15 min)
- [ ] Review Cost Support doc (5 min)
- [ ] Set up Claude account (5 min)

**During weeks 1-6:**
- [ ] Extract 10-20 papers
- [ ] Reply to 6 weekly check-ins
- [ ] Report any bugs clearly
- [ ] Try 2-3 edge case papers
- [ ] Participate in office hours (optional)

**End of week 6:**
- [ ] Spot-check 3-5 papers for accuracy
- [ ] Complete end-of-beta survey (20 min)
- [ ] Calculate your time savings
- [ ] Optional: Share testimonial

**Success:** You've completed the core tasks + been honest about what works and what doesn't.

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2025-10-24 | Initial expectations document with MVP status, limitations, commitments, and success criteria |

---

**Questions? Unclear on anything?** Ask in GitHub Discussions or Discord. We want you to succeed and feel supported.

**You've got this. Let's build MAestro together! üöÄ**
