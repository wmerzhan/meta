# Story 3.5: Qualitative Feedback Summary

**Study Period:** October 28 - December 6, 2025 (6 weeks)
**Testers:** 7 beta testers
**Data Collection:** End-of-beta interviews (5 sync calls, 2 async email surveys)
**Analysis Date:** December 10, 2025

---

## Overall Sentiment Distribution

| Sentiment Category | Count | Percentage | Tester IDs |
|-------------------|-------|------------|------------|
| **Very Positive** | 2 | 28.6% | Tester-02, Tester-05 |
| **Positive** | 3 | 42.9% | Tester-01, Tester-04, Tester-07 |
| **Neutral** | 1 | 14.3% | Tester-03 |
| **Negative** | 1 | 14.3% | Tester-06 |

**Summary:** 71.5% of testers expressed positive or very positive sentiment. One tester (Tester-06) had negative experience due to low technical comfort and medical domain complexity.

---

## Key Themes Identified

### Theme 1: Time Savings (Mentioned by 6/7 testers - 85.7%)

**Summary:** Nearly all testers reported significant time savings compared to manual extraction, despite learning curve and spot-checking requirements.

**Representative Quotes:**

> "MAestro saved me hours every week. Even with fact-checking the green labels, it's probably 50-60% faster than doing it all manually. I'd estimate 25 minutes per paper with MAestro versus 50+ minutes doing it by hand."
> — **Tester-02** (Public Health, Very Positive)

> "The first few papers took me just as long as manual extraction because I was learning the interface, but by week 3, I was flying through papers. I completed 15 papers in 6 weeks - no way I could have done that manually while teaching and doing other research."
> — **Tester-01** (Clinical Psychology, Positive)

> "I'm not a statistics person, so there was a learning curve understanding the output. But once I got it, MAestro definitely saved time compared to squinting at PDFs and copying numbers into Excel."
> — **Tester-03** (Education, Neutral)

**Analysis:** Time savings ranged from 30-60% across testers, with experienced meta-analysts (Tester-02, Tester-05) reporting higher efficiency gains. First-time users and non-technical testers took longer initially but saw improvements after 2-3 weeks.

---

### Theme 2: PDF Quality Issues (Mentioned by 5/7 testers - 71.4%)

**Summary:** Poor PDF quality (scanned papers, old PDFs, blurry tables) was the most common technical blocker, leading to yellow/red labels and increased manual review time.

**Representative Quotes:**

> "The biggest frustration was old scanned papers from the 1990s. MAestro would flag everything yellow or red because the OCR couldn't read the tables properly. I ended up doing those papers manually anyway."
> — **Tester-04** (Neuroscience, Positive)

> "I wish MAestro had warned me upfront when a PDF was low quality. Instead, I'd spend 20 minutes extracting, only to get mostly yellow/red labels and realize the paper wasn't readable for the AI."
> — **Tester-02** (Public Health, Very Positive)

> "Some of my older oncology papers were scanned PDFs with terrible image quality. MAestro struggled, and I couldn't trust the output. I would have appreciated a 'PDF quality too low - manual extraction recommended' warning."
> — **Tester-06** (Medical Research, Negative)

**Analysis:** PDF quality threshold handling emerged as a major edge case. Testers requested upfront PDF quality assessment before extraction begins.

---

### Theme 3: Accuracy Confidence and Three-Color Labeling (Mentioned by 7/7 testers - 100%)

**Summary:** All testers commented on the three-color labeling system. Most found it helpful for prioritizing review, though some initially misunderstood its meaning.

**Representative Quotes:**

> "The green/yellow/red labels were game-changers. I trusted green data about 90% of the time, spot-checked it for critical fields like effect size and sample size, and that worked well. Yellow meant 'definitely review this' and red meant 'probably wrong or missing.' Clear guidance."
> — **Tester-05** (Social Psychology, Very Positive, Expert Validator)

> "At first, I thought green meant 'definitely correct' so I didn't check anything. Then I found a couple errors in green-labeled data and realized green means 'high confidence' not 'perfect.' Once I understood that, I spot-checked green data and it was fine."
> — **Tester-01** (Clinical Psychology, Positive)

> "I didn't fully trust the labels. I checked everything regardless of color, which kind of defeated the purpose of the automation. Maybe I was overly cautious, but for publication-quality data, I can't rely on AI labels alone."
> — **Tester-03** (Education, Neutral)

> "The labeling was inconsistent. Some papers with great PDFs had yellow labels for no clear reason. Other times, MAestro was green on data that turned out to be wrong when I checked the paper. I lost confidence in the labels by week 4."
> — **Tester-06** (Medical Research, Negative)

**Analysis:** Trust in labels varied by tester experience and technical comfort. Experienced meta-analysts (Tester-02, Tester-05) used labels effectively as triage tools. Less experienced or lower-technical-comfort users either over-relied on labels or didn't trust them at all.

---

### Theme 4: Error Messages and Usability (Mentioned by 5/7 testers - 71.4%)

**Summary:** Error messages were frequently cited as unclear or unhelpful, especially for non-technical users.

**Representative Quotes:**

> "When something went wrong, the error messages were cryptic. I'd see 'Validation failed' but have no idea which field was the problem or how to fix it. I'd just try re-running the extraction and hope it worked."
> — **Tester-03** (Education, Neutral)

> "Error messages need to be more specific. Instead of 'Validation failed,' say 'Sample size field missing - please check Table 1 or Methods section.' Give me actionable guidance."
> — **Tester-02** (Public Health, Very Positive)

> "As someone without coding experience, I felt lost when errors happened. I couldn't troubleshoot because I didn't understand what went wrong. Better error explanations would help non-technical users like me."
> — **Tester-06** (Medical Research, Negative)

**Analysis:** Usability for non-technical users is a key area for improvement. Error messages need to be more descriptive and suggest next steps.

---

### Theme 5: Complex Study Designs (Mentioned by 4/7 testers - 57.1%)

**Summary:** Multi-arm trials, cluster RCTs, and non-standard designs were challenging for MAestro, often requiring manual intervention.

**Representative Quotes:**

> "MAestro struggled with multi-arm trials. My paper had 3 treatment arms and a control, and MAestro only extracted one effect size without telling me which comparison. I had to manually go back and extract all pairwise comparisons."
> — **Tester-01** (Clinical Psychology, Positive)

> "Cluster-randomized trials were hit or miss. Sometimes MAestro correctly identified the cluster design, other times it labeled it as a standard RCT. I had to double-check every cluster trial."
> — **Tester-05** (Social Psychology, Very Positive)

**Analysis:** Complex designs (multi-arm, cluster RCT, factorial) need improved prompt logic or user guidance to handle multiple effect sizes and design nuances.

---

### Theme 6: Domain-Specific Challenges (Mentioned by 3/7 testers - 42.9%)

**Summary:** Testers in specialized domains (neuroscience, oncology, environmental science) reported that MAestro was less familiar with domain-specific metrics.

**Representative Quotes:**

> "Neuroimaging outcomes like fMRI activation weren't well-extracted. MAestro labeled them yellow appropriately, which was honest, but it meant more manual work for me in my domain."
> — **Tester-04** (Neuroscience, Positive)

> "Environmental science uses different outcome metrics - carbon footprint, energy consumption, policy adoption. MAestro flagged these as yellow initially, but by the end, it seemed to 'learn' and I got more green labels. Not sure if that's real or just my perception."
> — **Tester-07** (Environmental Science, Positive)

**Analysis:** Domain-specific training data or user guidance for specialized fields (neuroimaging, oncology, environmental science) would improve confidence and reduce yellow labeling.

---

## Recommendation Rates

**Question:** "Would you recommend MAestro to a colleague conducting a meta-analysis?"

| Response | Count | Percentage | Tester IDs |
|----------|-------|------------|------------|
| **Yes, enthusiastically** | 2 | 28.6% | Tester-02, Tester-05 |
| **Yes, with caveats** | 4 | 57.1% | Tester-01, Tester-03, Tester-04, Tester-07 |
| **Maybe, depends on use case** | 0 | 0% | — |
| **No, not ready** | 1 | 14.3% | Tester-06 |

**Total "Would Recommend" (Yes + Yes with caveats):** 6/7 = **85.7%**

**Common Caveats:**
- "For exploration-grade data, not publication without verification" (Testers 01, 03, 04)
- "Only for well-formatted PDFs, not scanned papers" (Testers 01, 04, 07)
- "Best for experienced meta-analysts who can spot-check output" (Tester 03)
- "Needs better error messages for non-technical users" (Tester 03)

---

## Use Case Characterization

Based on qualitative feedback, testers identified MAestro as best suited for:

### Recommended Use Cases:

**1. Rapid Literature Scoping / Preliminary Exploration (6/7 testers)**
> "Perfect for getting a quick sense of the literature before committing to a full systematic review. I'd use this to decide if there's enough evidence to justify a meta-analysis."
> — **Tester-05**

**2. Time-Constrained Reviews with Verification (5/7 testers)**
> "If I had 100 papers to extract and limited time, I'd use MAestro to do the bulk extraction and then spot-check 20-30% for accuracy. Huge time saver."
> — **Tester-02**

**3. Training Tool for Meta-Analysis Novices (3/7 testers)**
> "As someone new to meta-analysis, MAestro helped me understand what data to look for in papers. It was like having a tutor showing me 'this is the effect size, this is the sample size, etc.'"
> — **Tester-03**

**4. Well-Formatted RCT Papers (7/7 testers)**
> "If the paper is a clean, modern RCT with clear tables and standard reporting, MAestro nails it. I got 100% green labels on several papers, and when I checked, they were perfect."
> — **Tester-01**

### Cautioned / Not Recommended Use Cases:

**1. Sole Data Source for Publication-Grade Meta-Analysis (5/7 testers)**
> "I wouldn't publish a meta-analysis based solely on MAestro's output without manual verification. It's good, but not perfect. I'd want a human to check every data point before submitting to a journal."
> — **Tester-04**

**2. Poor-Quality / Scanned PDFs (6/7 testers)**
> "Don't waste your time on scanned PDFs from the 1980s-90s. MAestro can't read them well, and you'll end up doing it manually anyway."
> — **Tester-06**

**3. Highly Specialized Domains Without Verification (3/7 testers)**
> "If you're in a niche field like neuroimaging or oncology, MAestro might not understand your outcome metrics. You'll need to review everything carefully."
> — **Tester-04**

**4. Non-Technical Users Without Training (2/7 testers)**
> "I struggled initially because I'm not a statistician and don't code. MAestro needs better onboarding and plain-language explanations for non-technical users."
> — **Tester-06**

---

## Discipline-Specific Insights

### Clinical Psychology (Tester-01)
- **Strengths:** RCT extraction, standard psychological outcomes (depression, anxiety)
- **Challenges:** Multi-arm trials, cluster RCTs
- **Recommendation:** "Great for clinical trials in psychology. Would use again."

### Public Health / Epidemiology (Tester-02)
- **Strengths:** Cardiovascular trials, standard epidemiological outcomes (RR, HR, OR)
- **Challenges:** Multi-site heterogeneity
- **Recommendation:** "Highly recommend for public health meta-analyses. Biggest time-saver I've encountered."

### Education Research (Tester-03)
- **Strengths:** Literacy interventions, standardized effect sizes
- **Challenges:** Statistical terminology accessibility, non-RCT designs
- **Recommendation:** "Useful with training, but non-technical users need better guidance."

### Neuroscience (Tester-04)
- **Strengths:** Cognitive trials, memory outcomes
- **Challenges:** Neuroimaging metrics (fMRI), poor-quality PDFs
- **Recommendation:** "Good for cognitive psychology RCTs, less reliable for neuroimaging."

### Social Psychology (Tester-05)
- **Strengths:** Attitude change, prejudice reduction, standard social psych outcomes
- **Challenges:** Mediation analysis, complex psychological constructs
- **Recommendation:** "Excellent tool. Would integrate into my meta-analysis workflow immediately."

### Medical Research / Oncology (Tester-06)
- **Strengths:** Survival analysis (HR), overall survival outcomes
- **Challenges:** Diagnostic accuracy, biomarker stratification, medical terminology for non-technical users
- **Recommendation:** "Not ready for clinical medicine without significant improvements. Too many errors on complex trials."

### Environmental Science (Tester-07)
- **Strengths:** Behavioral interventions, policy trials
- **Challenges:** Environmental metrics (carbon footprint, energy use), quasi-experimental designs
- **Recommendation:** "Surprisingly good for environmental science. Would use for exploratory reviews."

---

## Top Requested Features

Compiled from all 7 testers:

1. **Upfront PDF Quality Assessment** (5 testers)
   - Display PDF quality score before extraction begins
   - Warning: "This PDF may be too low quality for accurate extraction"

2. **Clearer Error Messages with Actionable Guidance** (5 testers)
   - Specify which field failed and why
   - Suggest where to find missing data (e.g., "Check Table 2 or supplemental materials")

3. **Plain Language Option for Non-Technical Users** (3 testers)
   - Translate statistical jargon into plain English
   - Example: "Cohen's d = standardized mean difference = how big the effect is"

4. **Multi-Arm Trial Handling** (3 testers)
   - Extract all pairwise comparisons, not just one
   - Clearly indicate which arms were compared

5. **Confidence Score per Field** (2 testers)
   - Instead of just green/yellow/red, show "Green (95% confidence)" or "Yellow (60% confidence)"
   - Help users calibrate trust in labels

6. **Auto-Compute Missing Statistics** (2 testers)
   - If paper reports SE but not CI, auto-compute CI
   - If paper reports OR and baseline risk, compute RR

7. **Domain-Specific Training / Customization** (2 testers)
   - Allow users to "teach" MAestro domain-specific outcome metrics
   - Example: "In oncology, PFS means progression-free survival"

---

## End-of-Beta Summary Ratings

| Question | Average Rating (1-5) | Distribution |
|----------|---------------------|--------------|
| **Overall satisfaction with MAestro** | 3.7 / 5 | 1 @ 2, 1 @ 3, 3 @ 4, 2 @ 5 |
| **Confidence in extracted data for real research use** | 3.4 / 5 | 1 @ 2, 2 @ 3, 3 @ 4, 1 @ 5 |
| **Would you use MAestro in your actual research workflow?** | Yes: 6/7 (85.7%) | "Yes": 2, "Yes with conditions": 4, "No": 1 |

**Conditions for Use (among "Yes with conditions" responses):**
- "With manual verification for publication" (3 testers)
- "For well-formatted PDFs only" (2 testers)
- "After improved error messages" (1 tester)
- "For exploratory work, not final meta-analysis" (2 testers)

---

## Conclusion

**Overall Assessment:** MAestro is **viable for beta launch** with identified areas for improvement. 71.5% positive sentiment, 85.7% recommendation rate, and 85.7% willingness to use in research workflow indicate strong product-market fit for exploration-grade meta-analysis and rapid literature reviews.

**Primary Strengths:**
- Significant time savings (30-60% reduction)
- Three-color labeling system effective for triage
- High accuracy on well-formatted RCTs (90%+ on gold standards)
- Useful across diverse disciplines

**Primary Weaknesses:**
- PDF quality handling (no upfront warning, poor OCR on scanned papers)
- Error message clarity (especially for non-technical users)
- Complex study designs (multi-arm trials, cluster RCTs)
- Domain-specific metrics (neuroimaging, oncology, environmental science)

**Recommended Next Steps (Story 3.6):**
1. Implement upfront PDF quality assessment (Priority: High - Score 6)
2. Improve error message specificity and guidance (Priority: High - Score 6)
3. Enhance multi-arm trial extraction logic (Priority: High - Score 6)
4. Add plain language option for non-technical users (Priority: Medium - Score 4)
5. Document known limitations and use case guidance (Priority: High)

**Launch Readiness:** **READY FOR CONTROLLED BETA LAUNCH** with documented caveats. Not yet ready for unrestricted public release without improvements to PDF quality handling, error messages, and complex design support.
