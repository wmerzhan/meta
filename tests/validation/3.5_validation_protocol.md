# Story 3.5 Validation Protocol

**Version:** 1.0
**Date:** 2025-10-24
**Status:** Active for Beta Testing

---

## 1. Gold Standard Methodology

### Definition
A "gold standard" is a trusted reference extraction of data from a research paper. This serves as the ground truth for comparing MAestro's AI-extracted data.

### Gold Standard Sources (Priority Order)

1. **Published Meta-analyses / Systematic Reviews**
   - Data already extracted and published in systematic review papers
   - Example: If a published meta-analysis of depression interventions extracted study data, use those extractions as gold standard
   - Advantage: Peer-reviewed, publicly available
   - Source documentation: Full citation to published review

2. **Tester's Own Manual Extraction**
   - Tester extracts the same paper manually (without MAestro) as baseline
   - Tester provides their extraction notes
   - Advantage: Directly comparable to tester's normal workflow
   - Source documentation: "Manual extraction by Tester ID on [date]"

3. **Published Data Repositories / Open Science Data**
   - If paper's data/codebook is available (e.g., OSF, GitHub)
   - Example: Study authors publish their extraction details
   - Advantage: Authoritative source
   - Source documentation: "Open Science Foundation / GitHub repository [link]"

4. **Peer-Reviewed Data Extraction by Colleague**
   - If tester has a colleague who previously extracted the same paper
   - Requires documentation of colleague's extraction methodology
   - Source documentation: "Extraction by [colleague name/role] on [date]"

### Gold Standard Collection Form

Each gold standard paper should include:

```
Gold Standard Submission Form
==============================

Paper ID: [Study authors, year]
Tester ID: [Anonymous ID, e.g., Tester-01]
Date Submitted: [YYYY-MM-DD]

SOURCE TYPE (check one):
  [ ] Published meta-analysis/systematic review
  [ ] Tester's manual extraction
  [ ] Published data repository (OSF/GitHub/etc)
  [ ] Peer-reviewed colleague extraction

SOURCE DOCUMENTATION:
  [Full citation, link, or extraction date]

CRITICAL FIELDS EXTRACTED:
  - Effect Size (type, value, 95% CI): ___________
  - Sample Size (total N, per-arm N): ___________
  - Study Design (RCT/Cohort/Case-Control/Other): ___________
  - Population (age range, disease status): ___________
  - Outcome Measured: ___________
  - Study Authors (lead/all): ___________
  - Year of Publication: ___________

NOTES ON EXTRACTION METHODOLOGY:
  [How was this extraction done? Any challenges? Edge cases?]

CONFIDENCE IN THIS GOLD STANDARD:
  [ ] High (published, peer-reviewed, or tester's direct extraction)
  [ ] Medium (published but secondary source)
  [ ] Low (partial data, unclear methodology)
```

---

## 2. Validation Methodology

### Critical Fields for Comparison

These fields are compared between MAestro extraction and gold standard:

| Field | Comparison Logic | Tolerance | Status |
|-------|------------------|-----------|--------|
| **Effect Size** | Exact numeric match or Â±5% if rounding difference | Â±5% for continuous, exact for categorical | ðŸŸ¢ if match |
| **Sample Size (Total N)** | Exact match required | 0 (exact) | ðŸŸ¢ if match |
| **Sample Size (Per Arm)** | Exact match required for RCT/comparative studies | 0 (exact) | ðŸŸ¢ if match |
| **Study Design** | Must match category (RCT, Cohort, Case-Control, Cross-sectional, etc.) | Exact classification | ðŸŸ¢ if match |
| **Primary Outcome** | Must match semantic meaning (e.g., both identify depression as outcome) | Exact or clear synonym | ðŸŸ¢ if match |
| **Study Authors** | Lead author minimum; full author list preferred | Lead author match minimum | ðŸŸ¢ if match |
| **Year of Publication** | Exact match | 0 (exact) | ðŸŸ¢ if match |
| **Population Characteristics** | General match acceptable (age range, disease status) | Â±5 years for age, same disease/condition | ðŸŸ¡ if close, ðŸ”´ if distinct |

### Field-by-Field Comparison Procedure

For each paper:

1. **Extract both datasets:** MAestro output + Gold standard side-by-side
2. **Score each field:** Match (âœ“) or Mismatch (âœ—)
3. **Document mismatch reason:**
   - Hallucination (MAestro invented data not in paper)
   - Missed data (data present in paper, not extracted by MAestro)
   - Misread (MAestro misinterpreted paper)
   - Ambiguous reporting (paper unclear, both interpretations reasonable)
4. **Label data quality:** Does this mismatch correspond to ðŸŸ¢/ðŸŸ¡/ðŸ”´ labeling?

### Example Comparison

```
PAPER: Smith et al., 2015 - Depression Intervention Trial

FIELD: Primary Effect Size (Depression Score Reduction)
  Gold Standard: HR = 1.45 (95% CI: 1.20-1.75)
  MAestro Output: HR = 1.45 (95% CI: 1.19-1.76)
  Tolerance: Â±5% = 1.38-1.53 for point estimate
  Result: âœ“ MATCH (CI slightly different due to rounding, within tolerance)
  Data Quality: ðŸŸ¢

FIELD: Study Design
  Gold Standard: Randomized Controlled Trial (2-arm parallel)
  MAestro Output: RCT with 2 arms
  Result: âœ“ MATCH
  Data Quality: ðŸŸ¢

FIELD: Total Sample Size
  Gold Standard: N = 480 (240 per arm)
  MAestro Output: N = 450
  Tolerance: 0 (exact match required)
  Result: âœ— MISMATCH - Hallucination? Paper actually reports N=480
  Data Quality: ðŸ”´ (if MAestro labeled it ðŸŸ¢, this is a false negative)
```

### Calculation: Paper-Level Accuracy

For each paper:
```
Accuracy % = (Matching Fields / Total Fields Scored) Ã— 100

Example:
  Total Fields: 8
  Matching: 7
  Mismatching: 1
  Accuracy: 7/8 = 87.5%
```

**Success criterion per paper:** â‰¥90% accuracy (all 8 fields match)

---

## 3. Sample Methodology

### Sample Selection Strategy

**Gold Standard Sample Size:**
- Minimum 3 papers per tester (if tester extracts 10-20 papers total)
- If tester extracts >20 papers, sample 15% (3-4 papers)
- Target: 15-50 gold standard papers across 5-10 testers

**Selection Method:**
- **Random Selection:** Testers randomly select 3+ papers from their batch before extraction
- **Stratification (if possible):**
  - 1 paper from study designs (RCT, Cohort, Case-Control mix)
  - 1 paper from different disciplines (if tester works cross-discipline)
  - 1 paper as "challenging" (poor PDF, complex design)

### Missing Gold Standard Handling

If tester cannot provide gold standard for planned papers:
- **Option A:** Tester substitutes a different paper with gold standard available
- **Option B:** Paper is extracted but excluded from accuracy analysis (flagged as "no gold standard")
- **Option C:** Extraction is peer-reviewed by another tester (lightweight peer review)

---

## 4. Edge Cases & Special Handling

### Multi-Arm Trials
- **Issue:** Paper may report multiple effect sizes (arm A vs. control, arm B vs. control)
- **Handling:** Extract all reported effect sizes; compare set of values rather than single value
- **Example:** Paper reports OR=1.5 (treat A) and OR=1.8 (treat B); both should appear in MAestro output

### Confounded Outcomes
- **Issue:** Outcome definition ambiguous (e.g., "depression" might mean PHQ-9 or HAM-D)
- **Handling:** Both testers and MAestro should note the specific outcome measure; match on measure, not just outcome name
- **Example:** If gold standard says "PHQ-9 score reduction" and MAestro says "depression improvement," this is flagged for manual review

### Different Statistical Reporting
- **Issue:** Paper reports OR but tester prefers RR; MAestro converts ORâ†’RR
- **Conversion Rule:** Use standard epidemiological conversions (RR â‰ˆ OR / (1 - baseline risk))
- **Handling:** If conversion is documented, both values marked as acceptable matches

### Updated/Corrected Data (Errata)
- **Issue:** Published meta-analysis later issues correction
- **Handling:** Use the corrected version as gold standard; document correction in tester notes
- **Example:** Meta-analysis reports N=500 initially, corrects to N=520 in erratum; use 520

### Non-English Papers
- **Issue:** MAestro may extract English field labels; comparison should still work
- **Handling:** Focus on data values (numbers, dates), not labels; verify semantic meaning
- **Example:** Spanish paper reporting "tamaÃ±o de la muestra" = sample size; match on value if numbers align

---

## 5. Standardized Data Validation Template

**File:** `3.5_gold_standard_comparison_[tester_id]_[paper_id].csv`

| Paper ID | Tester ID | Field | Gold Standard Value | MAestro Value | Match? | Tolerance Applied | Mismatch Type | MAestro Label (ðŸŸ¢/ðŸŸ¡/ðŸ”´) | Notes |
|----------|-----------|-------|---------------------|----------------|--------|-------------------|---------------|--------------------------|-------|
| Smith-2015 | Tester-01 | Effect Size | HR=1.45 | HR=1.45 | âœ“ | Â±5% | N/A | ðŸŸ¢ | Exact match |
| Smith-2015 | Tester-01 | Sample Size | 480 | 480 | âœ“ | 0 | N/A | ðŸŸ¢ | Exact match |
| Smith-2015 | Tester-01 | Design | RCT 2-arm | RCT | âœ“ | Exact | N/A | ðŸŸ¢ | Match |
| Jones-2018 | Tester-02 | Primary Outcome | Depression (PHQ-9) | Mood symptoms | âœ— | Semantic | Ambiguous reporting | ðŸŸ¡ | Requires review |

---

## 6. Data Quality Labeling Integration

As testers extract data, they assign ðŸŸ¢/ðŸŸ¡/ðŸ”´ labels per best practices.

During accuracy validation:
- **ðŸŸ¢ data (high confidence):** Should show â‰¥90% match with gold standard
- **ðŸŸ¡ data (uncertain/computed):** May have lower match rate (30-60%); expected to differ if tester's uncertainty was flagged
- **ðŸ”´ data (missing/failed):** Indicates extraction problem; comparison shows why

**False Positive / False Negative Tracking:**
- **False Positive (ðŸ”´ when correct):** If gold standard shows data IS correct, but MAestro labeled ðŸ”´, count as FP
- **False Negative (ðŸŸ¢ when wrong):** If gold standard shows data is WRONG, but MAestro labeled ðŸŸ¢, count as FN

---

## Timeline

- **Week 5:** Gold standards collected from testers (or identified from published sources)
- **Weeks 5-7:** Testers extract papers, MAestro generates data with ðŸŸ¢/ðŸŸ¡/ðŸ”´ labels
- **Week 7:** Accuracy validation begins (side-by-side comparisons)
- **Week 8:** Analysis complete, failure modes documented, report drafted

---

## References

- AC1-8 from Story 3.5: Conduct Validation Studies with Beta Testers
- Best Practices Guide (Story 3.2): Three-color labeling framework
- Epic 3 Requirements: Validation methodology specifications
