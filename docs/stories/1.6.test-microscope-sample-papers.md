# Story 1.6: Test Microscope with Sample Papers Across Disciplines

<!-- Powered by BMAD™ Core -->

## Status

**In Progress**

## Story

**As a** solo founder validating MVP assumptions,
**I want** to test Microscope v1.0 with 5-10 sample papers from different research disciplines,
**so that** I can identify failure modes, measure extraction time, and refine the prompt before beta testing with real users.

## Acceptance Criteria

1. Sample papers selected representing at least 3 disciplines (e.g., medicine, psychology, education) with varying complexity (simple RCT, meta-analysis, quasi-experimental study)
2. Each sample paper processed using Microscope prompt, generating complete data card following format standard
3. Extraction time measured and documented (target: <5 minutes per paper for typical 8-10 page journal articles)
4. Three-color labeling coverage analyzed: percentage of data points labeled 🟢/🟡/🔴 recorded to establish baseline
5. Known data extraction errors documented (at least 2-3 papers should have "gold standard" manual extraction for comparison, even if informal)
6. Failure modes identified and categorized (prompt misunderstanding, format violations, hallucinated data, context length issues)
7. Refinements to Microscope prompt documented and implemented based on testing results
8. Sample data cards added to `examples/` directory with accompanying source papers (if copyright permits) or detailed descriptions
9. Terminal output formatting validated with 2-3 non-technical users for readability: test markdown rendering in terminal, three-color emoji visibility across platforms (Windows Terminal, macOS Terminal.app, Linux terminals), error message clarity, progress indicator comprehensibility

## Tasks / Subtasks

- [x] **Task 1: Select representative sample papers for testing** (AC: 1)
  - [x] Identify 3 disciplines for testing (e.g., medicine, psychology, education)
  - [x] Select 5-10 papers total with varying characteristics:
    - [ ] At least 2 simple RCTs (8-10 pages, clear IMRAD structure)
    - [ ] At least 1 complex RCT (>12 pages, multiple outcomes, subgroup analyses)
    - [ ] At least 1 quasi-experimental or observational study
    - [ ] At least 1 meta-analysis or systematic review (to test exclusion workflow)
    - [ ] Varying word counts: 8,000-12,000 words (primary target), one 15,000+ words (stress test)
  - [ ] Ensure papers are diverse in:
    - [ ] Reporting quality (some high-quality, some with missing data)
    - [ ] Statistical complexity (simple t-tests to complex mixed models)
    - [ ] Table density (few tables vs. many tables)
  - [ ] Document paper selection criteria and rationale
  - [ ] Store papers in `examples/sample_meta_analysis/source_papers/` (if copyright permits) or create detailed descriptions

- [ ] **Task 2: Create gold standard extractions for validation subset** (AC: 5)
  - [ ] Select 2-3 papers from sample for "gold standard" manual extraction
  - [ ] Manually extract data from these papers WITHOUT using Microscope
  - [ ] Create gold standard data cards following `templates/data_card.md` format
  - [ ] Store gold standard extractions in `tests/validation/gold_standards/`
  - [ ] Document extraction decisions and rationale
  - [ ] These will be compared against Microscope-generated extractions to identify errors

- [ ] **Task 3: Process each sample paper using Microscope v1.0** (AC: 2, 3)
  - [ ] For each sample paper:
    - [ ] Open Claude Code and start new conversation
    - [ ] Load `prompts/microscope/microscope_v1.0.md` prompt
    - [ ] Attach or paste research paper
    - [ ] Start timer for extraction time measurement
    - [ ] Let Claude Code complete extraction workflow (screening → quality → data)
    - [ ] Stop timer when data card is generated
    - [ ] Record extraction time in testing log
  - [ ] Save each generated data card to `examples/sample_meta_analysis/data_cards/`
  - [ ] Name files using study_id from data card (e.g., `smith_2023_rct.md`)
  - [ ] Create extraction time summary table:
    - Paper ID | Discipline | Word Count | Pages | Extraction Time | Notes

- [ ] **Task 4: Analyze three-color labeling coverage** (AC: 4)
  - [ ] For each generated data card, count data points by label type:
    - [ ] Count 🟢 (green/direct quote) data points
    - [ ] Count 🟡 (yellow/computed) data points
    - [ ] Count 🔴 (red/uncertain) data points
    - [ ] Count total data points extracted
  - [ ] Calculate percentages for each label type per paper
  - [ ] Calculate aggregate statistics across all papers:
    - [ ] Mean percentage 🟢/🟡/🔴
    - [ ] Distribution patterns (do certain paper types have more 🔴?)
  - [ ] Create labeling coverage summary table
  - [ ] Document baseline three-color labeling patterns
  - [ ] Assess if 90%+ uncertain data flagging goal is being met [Source: docs/data-card-format.md#three-color-labeling-rules]

- [ ] **Task 5: Validate extraction accuracy against gold standards** (AC: 5)
  - [ ] For papers with gold standard extractions:
    - [ ] Compare Microscope-generated data card to gold standard
    - [ ] Identify discrepancies:
      - [ ] Incorrect values (wrong numbers)
      - [ ] Missing data points (gold standard has it, Microscope missed)
      - [ ] Hallucinated data (Microscope extracted data not in paper)
      - [ ] Incorrect source labels (wrong 🟢🟡🔴 classification)
      - [ ] Missing evidence (labels without page refs/calculations)
      - [ ] Quality assessment disagreements
    - [ ] Categorize each error by type and severity
    - [ ] Document error rate per paper
  - [ ] Create extraction error summary:
    - Error type | Frequency | Severity (critical/moderate/minor) | Example | Suggested fix
  - [ ] Save validation results to `tests/validation/1.6_microscope_testing_results.md`

- [ ] **Task 6: Identify and categorize failure modes** (AC: 6)
  - [ ] Review all generated data cards for failure modes:
    - [ ] **Prompt misunderstanding**: Did Claude Code follow extraction workflow correctly?
    - [ ] **Format violations**: YAML syntax errors, malformed tables, missing sections
    - [ ] **Hallucinated data**: Values not present in paper
    - [ ] **Context length issues**: Truncation, incomplete extraction for long papers
    - [ ] **Quality assessment errors**: Incorrect domain ratings, missing justifications
    - [ ] **Source label errors**: Wrong labels, missing evidence
    - [ ] **Calculation errors**: Incorrect effect size computations
  - [ ] For each failure mode identified:
    - [ ] Document specific example from testing
    - [ ] Categorize severity (blocks usage vs. minor issue)
    - [ ] Propose root cause
    - [ ] Suggest mitigation strategy
  - [ ] Create failure mode catalog in `tests/validation/failure_modes.md`

- [ ] **Task 7: Refine Microscope prompt based on testing results** (AC: 7)
  - [ ] Analyze failure modes and errors to identify prompt improvements
  - [ ] Draft refinements to address common issues:
    - [ ] Clarify ambiguous instructions
    - [ ] Add examples for common error patterns
    - [ ] Strengthen source labeling evidence requirements
    - [ ] Improve quality assessment guidance
  - [ ] Create `prompts/microscope/microscope_v1.1.md` (if refinements are substantial)
    - [ ] OR document minor refinements as "known issues" for v1.0
  - [ ] Update `prompts/microscope/CHANGELOG.md` with testing findings and changes
  - [ ] Re-test refined prompt on 1-2 papers to validate improvements

- [ ] **Task 8: Add sample data cards and documentation to examples/** (AC: 8)
  - [ ] Copy tested data cards to `examples/sample_meta_analysis/data_cards/`
  - [ ] If papers are public domain or CC-licensed:
    - [ ] Copy paper PDFs to `examples/sample_meta_analysis/source_papers/`
  - [ ] If papers are copyrighted:
    - [ ] Create `examples/sample_meta_analysis/source_papers/README.md` describing papers with:
      - [ ] Full citation
      - [ ] DOI or access link
      - [ ] Brief description of why this paper was selected
      - [ ] Key characteristics (discipline, design, complexity)
  - [ ] Create `examples/sample_meta_analysis/README.md` explaining:
    - [ ] Purpose of these examples
    - [ ] How to use them to learn MAestro workflow
    - [ ] Labeling coverage statistics for reference
  - [ ] Ensure example data cards demonstrate:
    - [ ] Range of three-color labels (🟢🟡🔴)
    - [ ] Quality assessment across all domains
    - [ ] Both included and excluded screening decisions
    - [ ] Various study designs and disciplines

- [ ] **Task 9: Validate terminal output formatting with non-technical users** (AC: 9)
  - [ ] Recruit 2-3 non-technical test users (not developers, ideally researchers)
  - [ ] Prepare test scenario:
    - [ ] Provide one sample paper
    - [ ] Provide Microscope prompt
    - [ ] Ask user to run extraction in Claude Code
    - [ ] Ask user to save resulting data card
  - [ ] Test on multiple platforms:
    - [ ] Windows Terminal (Windows 10+)
    - [ ] macOS Terminal.app
    - [ ] Linux terminal (Ubuntu or similar)
  - [ ] Evaluate readability:
    - [ ] Can users see three-color emojis (🟢🟡🔴) clearly?
    - [ ] Is markdown formatting rendering correctly?
    - [ ] Are tables readable in terminal output?
    - [ ] Are YAML blocks clearly distinguished?
  - [ ] Gather feedback:
    - [ ] Was output confusing at any point?
    - [ ] Were instructions clear?
    - [ ] Did they successfully save data card?
    - [ ] Did they understand the three-color labeling system?
  - [ ] Document user feedback in `tests/validation/terminal_output_user_testing.md`
  - [ ] Identify any formatting improvements needed
  - [ ] Update `docs/microscope-usage.md` if user testing reveals documentation gaps

- [ ] **Task 10: Create comprehensive testing summary report** (Not explicit AC, but essential)
  - [ ] Compile all testing results into `tests/validation/1.6_testing_summary.md`:
    - [ ] Papers tested (table with disciplines, characteristics, outcomes)
    - [ ] Extraction time statistics (mean, median, range, per paper breakdown)
    - [ ] Labeling coverage statistics (🟢🟡🔴 percentages)
    - [ ] Extraction accuracy results (comparison to gold standards)
    - [ ] Failure modes catalog (types, frequencies, mitigations)
    - [ ] Prompt refinements implemented
    - [ ] Terminal formatting validation results
    - [ ] Overall assessment: Is Microscope v1.0 ready for beta testing?
  - [ ] Create visual summary:
    - [ ] Extraction time distribution (if possible)
    - [ ] Labeling coverage pie chart (if possible)
    - [ ] Error rate summary
  - [ ] Document recommendations for next steps:
    - [ ] Are any blocking issues preventing beta launch?
    - [ ] What refinements are required before Story 1.7 (CI/CD)?
    - [ ] What known limitations should be documented for users?

## Dev Notes

### Story Context

Story 1.6 is the **validation and quality assurance story** that closes Epic 1 by rigorously testing the Microscope data extraction workflow before releasing to users. This is the first real-world stress test of MAestro's core value proposition: AI-assisted transparent research data extraction.

**Critical Importance:**
- **Validates MVP assumptions**: Does Microscope actually work across diverse papers?
- **Establishes quality baselines**: What's the expected accuracy and labeling coverage?
- **Identifies edge cases**: What failure modes need documentation or fixes?
- **Builds trust**: Demonstrable testing creates confidence for academic users
- **Prepares for beta**: Ensures tool is ready for external researchers

**Testing Philosophy** [Source: architecture/test-strategy-and-standards.md#testing-philosophy]:
- **Validation prioritized over automation in MVP phase**
- Manual testing is appropriate for this phase (automated testing in CROS)
- Focus on "gold standard driven" validation with manual comparison
- Cross-platform testing is required (Windows/macOS/Linux)

### Previous Story Insights

**From Story 1.1 (Repository Initialization):**
- Repository structure established with `examples/`, `tests/`, `prompts/` directories
- Git initialized for version control of all project files
- Examples directory ready for sample data cards [Source: architecture/source-tree.md]

**From Story 1.2 (Data Card Format):**
- Data card template defined at `templates/data_card.md`
- Three-color labeling system (🟢🟡🔴) is core differentiator
- YAML frontmatter + markdown body structure
- Validation checklist provided for manual quality control [Source: docs/data-card-format.md]

**From Story 1.3 (Generic Quality Checklist):**
- Generic quality checklist created at `modules/generic/generic_quality_checklist.md`
- 5 quality domains: Selection Bias, Measurement Validity, Confounding Control, Attrition/Missing Data, Reporting Transparency
- Quality ratings: Low/Medium/High risk
- Overall quality synthesis: High/Medium/Low [Source: modules/generic/generic_quality_checklist.md]

**From Story 1.4 (Microscope Prompt):**
- Microscope v1.0 prompt template created at `prompts/microscope/microscope_v1.0.md`
- Prompt tested with 8,000-12,000 word papers, IMRAD structure, quantitative studies
- Compatible with Claude Sonnet 4.5 (200K context window)
- Three-stage workflow: Screening → Quality Assessment → Data Extraction
- Source labeling evidence requirements clearly defined [Source: prompts/microscope/microscope_v1.0.md]

**From Story 1.5 (Git Integration):**
- Git workflows documented for data card version control
- Commit message conventions for research workflows
- Branching strategy for extraction batches
- Cross-platform Git considerations (line endings, path separators) [Source: docs/git-integration.md]

**Key Design Principles:**
- This is **empirical validation** not theoretical planning
- Must test with **real research papers** across **diverse characteristics**
- **Gold standard comparison** is essential for credibility
- **Cross-platform terminal testing** required for accessibility
- **Non-technical user feedback** validates usability assumptions

### MAestro Architecture Context

**Local-First Architecture** [Source: architecture/high-level-architecture.md#pattern-4-local-first-with-cloud-augmentation]
- All testing performed locally (no cloud infrastructure in MVP)
- Data cards stored as files in `examples/` directory
- Git used for version control of test results
- No database required for MVP testing

**Template-Driven Workflow** [Source: architecture/high-level-architecture.md#pattern-1-template-driven-workflow-orchestration]
- Microscope prompt template is the orchestrator
- Testing validates template effectiveness
- Versioning critical: Any refinements create v1.1 with CHANGELOG entry
- Never overwrite existing prompt versions [Source: architecture/coding-standards.md#rule-3-prompt-模板版本化]

**Three-Color Source Labeling System** [Source: architecture/high-level-architecture.md#pattern-3-three-color-source-labeling-system]
- 🟢 Green: Direct quote/observation (page + section reference)
- 🟡 Yellow: Computed/inferred value (calculation + source pages)
- 🔴 Red: Uncertain/missing data (explanation + impact)
- **CRITICAL TESTING FOCUS**: Measure labeling coverage (target: 90%+ uncertain data flagged)
- Labels must have evidence; this is non-negotiable for academic credibility [Source: docs/data-card-format.md#three-color-labeling-rules]

**Data Card Microservice Architecture** [Source: architecture/high-level-architecture.md#pattern-2-data-card-microservice-architecture]
- Each data card is atomic, independent unit
- Test data cards in `examples/` demonstrate format
- Failed extractions don't corrupt project state
- Files can be edited, versioned, validated independently

**Testing Strategy** [Source: architecture/test-strategy-and-standards.md]
- **MVP Phase**: Manual testing, validation prioritized over automation
- **Coverage goals**: Not applicable in MVP (no code yet)
- **Test organization**: Store validation results in `tests/validation/`
- **Cross-platform testing**: Required for Windows, macOS, Linux
- **User testing**: Required for terminal output validation

### Testing Methodology

**Test Paper Selection Criteria:**

Based on Microscope v1.0 design constraints and architecture:

1. **Word Count Range**: 8,000-12,000 words (primary target) [Source: prompts/microscope/microscope_v1.0.md]
   - Compatible with Claude Sonnet 4.5 200K context window
   - One stress test paper at 15,000+ words to test context limits

2. **Study Designs** (minimum 3 types):
   - Simple RCT (clear methods, straightforward results)
   - Complex RCT (multiple outcomes, subgroup analyses, long tables)
   - Observational/quasi-experimental (to test non-randomized design handling)
   - Meta-analysis/review (to test exclusion workflow)

3. **Disciplines** (minimum 3):
   - Medicine (clinical trials, interventions)
   - Psychology (behavioral interventions, psychological measures)
   - Education (learning interventions, academic outcomes)
   - Consider adding: Public Health, Social Work, Nutrition

4. **Reporting Quality Variation**:
   - High-quality papers (complete reporting, published in high-impact journals)
   - Medium-quality papers (some missing data, typical journal article)
   - Low-quality papers (substantial missing data, poor reporting)
   - This tests the 🔴 labeling system's ability to flag uncertainty

5. **Statistical Complexity**:
   - Simple analyses (t-tests, ANOVAs, basic regression)
   - Complex analyses (mixed models, structural equation models, multiple comparisons)
   - Papers with and without reported effect sizes (tests 🟡 calculation workflow)

**Gold Standard Creation Process:**

For 2-3 papers, manually extract data WITHOUT Microscope to create comparison baseline:

1. Read paper thoroughly
2. Fill out `templates/data_card.md` manually
3. Apply three-color labels based on personal judgment
4. Document extraction time (will likely be much longer than Microscope)
5. Store in `tests/validation/gold_standards/{study_id}.md`
6. Use for error detection in Task 5

**Error Classification Taxonomy:**

When comparing Microscope extractions to gold standards:

| Error Type | Definition | Severity | Example |
|------------|------------|----------|---------|
| **Incorrect Value** | Wrong number extracted | Critical | Sample size: Microscope says 120, paper says 142 |
| **Missing Data Point** | Gold standard has it, Microscope didn't extract | Moderate | Baseline age SD missing from Microscope extraction |
| **Hallucinated Data** | Microscope extracted data not in paper | Critical | Effect size calculated with non-existent values |
| **Wrong Source Label** | Incorrect 🟢🟡🔴 classification | Moderate | Used 🟢 for calculated value (should be 🟡) |
| **Missing Evidence** | Label without page ref/calculation | Moderate | 🟢 label but no page number provided |
| **Calculation Error** | Incorrect effect size computation | Critical | Cohen's d formula applied incorrectly |
| **Quality Rating Error** | Domain rated incorrectly | Minor | Selection bias rated Low when should be Medium |
| **Format Violation** | YAML syntax error, malformed table | Moderate | Missing `---` delimiter in frontmatter |

**Extraction Time Measurement Protocol:**

[Source: Epic AC #3: Target <5 minutes per typical 8-10 page paper]

1. Start timer when Claude Code conversation begins (after pasting prompt + paper)
2. Stop timer when complete data card is generated in Claude Code output
3. Do NOT include time for:
   - Reading/preparing the paper before Claude Code
   - Saving the data card file
   - Manual corrections or validation
4. Record exact time in minutes:seconds
5. Calculate statistics:
   - Mean extraction time across all papers
   - Median extraction time
   - Range (min/max)
   - Time by paper length (correlation)
   - Time by complexity (simple vs. complex)

**Three-Color Labeling Coverage Analysis:**

[Source: Epic AC #4, PRD goal: 90%+ uncertain data flagging]

For each data card, count:

```
Total Data Points = Count of all rows in all data tables
🟢 Count = Rows with green labels (direct quotes)
🟡 Count = Rows with yellow labels (computed values)
🔴 Count = Rows with red labels (uncertain/missing)

Percentages:
🟢% = (🟢 Count / Total Data Points) × 100
🟡% = (🟡 Count / Total Data Points) × 100
🔴% = (🔴 Count / Total Data Points) × 100

Quality Metric: Are 🔴 labels capturing uncertainty appropriately?
```

**Aggregate across all papers:**
- Mean 🟢%, 🟡%, 🔴% across papers
- Do papers with more missing data have higher 🔴%? (expected)
- Do high-quality papers have lower 🔴%? (expected)
- Are computed effect sizes always 🟡? (should be)

**Terminal Output Validation Process:**

[Source: Epic AC #9]

Recruit 2-3 non-technical users (researchers without programming background):

**Test Scenario:**
1. Provide written instructions (based on `docs/microscope-usage.md`)
2. Provide one sample paper (select a medium-complexity RCT)
3. Ask user to:
   - Open Claude Code
   - Load Microscope prompt
   - Attach paper
   - Complete extraction
   - Save data card to file
4. Observe user during process (screen share or in-person)
5. Ask user to "think aloud" during extraction

**Evaluation Criteria:**

| Aspect | Question | Pass/Fail |
|--------|----------|-----------|
| **Emoji Visibility** | Can user clearly see 🟢🟡🔴 in terminal? | Pass if all 3 visible |
| **Markdown Rendering** | Are tables readable? Headers clear? | Pass if tables aligned |
| **YAML Clarity** | Can user identify YAML frontmatter section? | Pass if recognizes `---` blocks |
| **Error Messages** | If errors occur, are they understandable? | Pass if user can interpret |
| **Instructions** | Could user complete task following docs? | Pass if successful extraction |
| **Confusion Points** | Where did user get stuck or confused? | Document all confusion |

**Cross-Platform Testing:**

Test on each platform:
- **Windows**: Windows 10+ with Windows Terminal (NOT old cmd.exe)
- **macOS**: macOS Terminal.app (standard terminal)
- **Linux**: Ubuntu or similar with default terminal emulator

Check:
- Emoji rendering (some terminals don't support color emojis)
- Table alignment (depends on font/terminal width)
- YAML readability
- Copy-paste functionality (for saving data cards)

**Failure Mode Documentation:**

For each failure identified during testing:

```markdown
### Failure Mode: [Brief Name]

**Description:** [What went wrong]

**Frequency:** [How often occurred across test papers]

**Severity:** [Critical/Moderate/Minor]

**Example:**
[Concrete example from testing]

**Root Cause:** [Why it happened]

**Mitigation:**
- Short-term: [Workaround for users]
- Long-term: [Prompt refinement or documentation update]

**Related ACs:** [Which acceptance criteria affected]
```

Categories:
1. **Prompt Misunderstanding**: Claude Code didn't follow extraction workflow
2. **Format Violations**: Output doesn't match data card template
3. **Hallucinations**: Data not present in source paper
4. **Context Length**: Paper too long for context window
5. **Quality Assessment**: Incorrect domain ratings or justifications
6. **Source Labeling**: Wrong labels or missing evidence
7. **Calculations**: Effect size computations incorrect

### File Locations and Naming

[Source: architecture/source-tree.md]

**Test Data Storage:**
```
tests/
├── validation/                          # Manual validation test results
│   ├── 1.6_testing_summary.md          # Overall summary report
│   ├── 1.6_microscope_testing_results.md  # Detailed validation results
│   ├── failure_modes.md                # Failure mode catalog
│   ├── terminal_output_user_testing.md # User testing feedback
│   └── gold_standards/                 # Gold standard manual extractions
│       ├── {study_id}_gold.md
│       └── README.md
```

**Sample Data Cards:**
```
examples/
└── sample_meta_analysis/
    ├── README.md                        # Explains purpose of examples
    ├── data_cards/                      # Tested data cards from Story 1.6
    │   ├── {study_id}.md
    │   └── README.md
    ├── source_papers/                   # Original papers (if copyright permits)
    │   ├── {paper_name}.pdf
    │   └── README.md                    # Citations if papers not included
    ├── compiled/                        # Empty for now (Epic 2)
    └── analyses/                        # Empty for now (Epic 3)
```

**Prompt Refinements:**
```
prompts/
└── microscope/
    ├── microscope_v1.0.md               # Original (don't modify)
    ├── microscope_v1.1.md               # Refined version (if substantial changes)
    ├── CHANGELOG.md                     # Document testing findings and changes
    └── README.md
```

### Prompt Versioning Workflow

[Source: architecture/coding-standards.md#rule-3-prompt-模板版本化]

**If testing reveals need for refinements:**

```bash
# NEVER overwrite existing version
# Create new version:
cp prompts/microscope/microscope_v1.0.md prompts/microscope/microscope_v1.1.md

# Edit microscope_v1.1.md with refinements

# Update CHANGELOG.md:
echo "## v1.1 (2025-10-XX)
- Refinement: Clarified source label evidence requirements based on Story 1.6 testing
- Fix: Added explicit instruction to show intermediate calculation steps for effect sizes
- Improvement: Enhanced quality assessment guidance for confounding control domain
- Testing: Validated with 10 papers across 3 disciplines (medicine, psychology, education)
" >> prompts/microscope/CHANGELOG.md

# Update metadata in microscope_v1.1.md:
# - Version: 1.1
# - Created: 2025-10-XX
# - Changelog: See CHANGELOG.md

# Commit both files:
git add prompts/microscope/microscope_v1.1.md prompts/microscope/CHANGELOG.md
git commit -m "feat: Create Microscope v1.1 with refinements from Story 1.6 testing"
```

**If only minor issues (documentation, not prompt changes):**
- Document as "known limitations" in `docs/microscope-usage.md`
- Add to troubleshooting section
- No new prompt version needed

### Cross-Platform Terminal Compatibility

[Source: architecture/coding-standards.md#rule-7-路径必须跨平台兼容, architecture/tech-stack.md]

**Emoji Support:**

| Platform | Terminal | Emoji Support | Notes |
|----------|----------|---------------|-------|
| **Windows** | Windows Terminal | ✅ Full support | Recommended for Windows users |
| **Windows** | cmd.exe (old) | ❌ Limited | Unicode may not render; discourage use |
| **Windows** | PowerShell | ✅ Full support | Good alternative to Windows Terminal |
| **macOS** | Terminal.app | ✅ Full support | Default terminal supports emojis |
| **macOS** | iTerm2 | ✅ Full support | Alternative with better rendering |
| **Linux** | GNOME Terminal | ✅ Full support | Most modern Linux terminals support |
| **Linux** | xterm (old) | ⚠️ Partial | May show black-and-white emojis |

**If emoji rendering issues discovered:**
- Document in `docs/microscope-usage.md` under "Troubleshooting"
- Recommend modern terminals (Windows Terminal, macOS Terminal.app, GNOME Terminal)
- Consider fallback: Use text labels in addition to emojis (e.g., `🟢 [GREEN]`)

**Markdown Table Rendering:**

Claude Code outputs markdown tables, but terminal width affects readability:

```markdown
| Variable | Value | Source Label & Evidence |
|----------|-------|------------------------|
| Sample size | 120 | 🟢 (p. 7, Methods) |
```

If tables don't align in terminal:
- Likely due to narrow terminal width
- Instruct users to widen terminal window
- Document in usage guide: "Recommended terminal width: 120+ characters"

### Documentation Updates from Testing

Based on testing outcomes, update:

**`docs/microscope-usage.md`:**
- Add "Common Issues" section for identified failure modes
- Add terminal compatibility notes if issues discovered
- Update expected extraction times based on actual measurements
- Add examples of typical labeling coverage (e.g., "expect 40% 🟢, 30% 🟡, 30% 🔴")

**`docs/data-card-format.md`:**
- No changes expected (format is stable)
- If format violations discovered, clarify ambiguous sections

**`modules/generic/generic_quality_checklist.md`:**
- If quality assessment errors common, clarify rating guidance
- Add examples from tested papers

**`README.md`:**
- Update with "Sample Data Cards" section linking to `examples/`
- Add extraction time statistics as a reference

### Integration with Future Stories

**Story 1.7 (CI/CD Pipeline):**
- Testing validates that Microscope is ready for CI/CD automation
- Sample data cards provide test fixtures for CI validation workflows
- Failure modes inform what to check in automated validation

**Epic 2 (Compiler):**
- Sample data cards become input for Compiler testing
- Labeling coverage statistics inform Compiler's label propagation logic
- Diverse paper types ensure Compiler handles heterogeneous data

**Epic 3 (Documentation & Validation):**
- Testing results demonstrate RAAA (Reproducible AI-Assisted Analysis)
- Gold standard comparisons establish accuracy baselines for academic credibility
- Sample data cards serve as examples in RAAA documentation appendices

### Deliverables Summary

By end of this story, the following files/artifacts should exist:

1. **Test Results** (`tests/validation/`):
   - `1.6_testing_summary.md` - Overall summary report
   - `1.6_microscope_testing_results.md` - Detailed validation findings
   - `failure_modes.md` - Catalog of identified issues
   - `terminal_output_user_testing.md` - User testing feedback
   - `gold_standards/{study_id}_gold.md` - Manual extractions for comparison

2. **Sample Data Cards** (`examples/sample_meta_analysis/data_cards/`):
   - 5-10 data cards from tested papers
   - README.md explaining purpose and characteristics

3. **Source Papers** (`examples/sample_meta_analysis/source_papers/`):
   - PDFs (if copyright permits) OR
   - README.md with citations and access information

4. **Prompt Refinements** (`prompts/microscope/`):
   - `microscope_v1.1.md` (if substantial refinements) OR
   - Updated `CHANGELOG.md` with testing notes (if minor/no changes)

5. **Documentation Updates**:
   - `docs/microscope-usage.md` - Updated troubleshooting and usage guidance
   - `README.md` - Updated with testing results and sample data cards link

6. **Extraction Statistics**:
   - Extraction time measurements (target: <5 min for 8-10 page papers)
   - Labeling coverage percentages (🟢🟡🔴 distribution)
   - Accuracy metrics (comparison to gold standards)

All files committed to Git with descriptive commit messages.

### Success Criteria for Story Completion

This story is considered **Done** when:

- [ ] All 9 acceptance criteria met
- [ ] 5-10 sample papers tested across 3+ disciplines
- [ ] Extraction times measured and documented (with assessment vs. <5 min target)
- [ ] Labeling coverage analyzed and baseline established
- [ ] Gold standard comparisons completed with error documentation
- [ ] Failure modes identified, categorized, and mitigated
- [ ] Prompt refinements (if needed) implemented and versioned
- [ ] Sample data cards added to `examples/` with documentation
- [ ] Terminal output validated on Windows/macOS/Linux with non-technical users
- [ ] Comprehensive testing summary report created
- [ ] All deliverables committed to Git

**Quality Gate:**
- At least 70% of tested papers generate valid data cards (YAML parses, required fields present)
- Mean extraction time ≤ 7 minutes (allowing 2-minute buffer over 5-minute target)
- Labeling coverage shows 🔴 labels being applied (confirms uncertainty flagging works)
- No critical blocking issues preventing beta user testing

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-21 | 1.0 | Initial story creation with comprehensive testing methodology and architecture integration | SM Agent (Bob) |

## Dev Agent Record

### Agent Model Used

claude-sonnet-4-5-20250929

### Debug Log References

N/A - Testing infrastructure setup phase

### Completion Notes List

**2025-10-21 - Testing Infrastructure Created**

Created comprehensive testing framework and documentation templates to support empirical validation of Microscope v1.0. All infrastructure files are ready for actual testing once research papers are obtained.

**Files created:**
1. `tests/validation/paper_selection_criteria.md` - Systematic criteria for selecting representative test papers
2. `tests/validation/gold_standards/README.md` - Gold standard extraction documentation
3. `tests/validation/extraction_time_log.md` - Template for tracking extraction times
4. `tests/validation/labeling_coverage_analysis.md` - Three-color labeling analysis framework
5. `tests/validation/1.6_microscope_testing_results.md` - Comprehensive results template
6. `tests/validation/failure_modes.md` - Failure mode catalog template
7. `tests/validation/terminal_output_user_testing.md` - User testing protocol and results template
8. `examples/sample_meta_analysis/source_papers/README.md` - Paper documentation template

**Current Status: BLOCKED**

This story requires empirical testing with actual research papers. The testing infrastructure is complete, but proceeding requires:

1. **5-10 research papers** selected according to criteria in `tests/validation/paper_selection_criteria.md`
2. Papers must cover:
   - At least 3 disciplines (medicine, psychology, education)
   - Mix of study designs (simple RCT, complex RCT, quasi-experimental, meta-analysis)
   - Word count range: 8,000-12,000 words (standard), one 15,000+ (stress test)
   - Variety of reporting quality levels

**Next Steps:**
- User provides research papers OR
- User directs how to proceed with testing (simulated vs. actual papers)

All testing templates are structured to document:
- Paper characteristics and selection rationale
- Extraction times (target: <5 minutes)
- Three-color labeling coverage (🟢🟡🔴 percentages)
- Accuracy vs. gold standards
- Failure modes and mitigations
- Terminal output usability across platforms

### File List

**Created:**
- `tests/validation/paper_selection_criteria.md`
- `tests/validation/gold_standards/README.md`
- `tests/validation/extraction_time_log.md`
- `tests/validation/labeling_coverage_analysis.md`
- `tests/validation/1.6_microscope_testing_results.md`
- `tests/validation/failure_modes.md`
- `tests/validation/terminal_output_user_testing.md`
- `examples/sample_meta_analysis/source_papers/README.md`

## QA Results

*This section will be populated by the QA agent during review*
