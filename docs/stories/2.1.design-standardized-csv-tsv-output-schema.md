# Story 2.1: Design Standardized CSV/TSV Output Schema

<!-- Powered by BMAD‚ÄØCore -->

## Status

**Done**

## Story

**As a** researcher preparing to analyze Meta-analysis data,  
**I want** a well-defined, standardized CSV/TSV output format from the Compiler,  
**so that** I can import aggregated data seamlessly into R, Python, Excel, or statistical software without manual reformatting.

## Acceptance Criteria

1. CSV/TSV schema documented (`docs/compiled-data-schema.md`) defining required columns: study_id, authors, year, sample_size, intervention, comparison, outcome_measure, effect_size, confidence_interval_lower, confidence_interval_upper, quality_score, source_color_label
2. Schema includes metadata columns for data provenance: data_card_file, extraction_date, extractor_name
3. Schema accommodates multiple outcome types: continuous (mean difference, standardized mean difference), dichotomous (odds ratio, risk ratio), correlation coefficients
4. Documentation explains how three-color labels are preserved in compiled dataset (source_color_label column with üü¢/üü°/üî¥ values)
5. Example compiled CSV file provided (`examples/sample_compiled_data.csv`) showing aggregated results from 5-10 data cards
6. Schema validated for compatibility with R's `readr::read_csv()` and Python's `pandas.read_csv()` (test scripts provided)
7. Guidance on handling missing values and heterogeneous data structures included (e.g., when papers report different effect size metrics)

## Project Structure Notes

- Schema documentation must live under `docs/` alongside other architecture-approved references; place new file at `docs/compiled-data-schema.md` per repository documentation conventions. [Source: architecture/source-tree.md#source-tree]
- Example compiled outputs belong in `examples/`, with CSV artifacts surfacing under `examples/sample_meta_analysis/compiled/` to align with the sample project scaffold. [Source: architecture/source-tree.md#source-tree]
- Test scripts validating parser compatibility should be stored inside `tests/validation/` (or a new `tests/integration/` folder if stronger separation is required), matching the canonical testing layout. [Source: architecture/test-strategy-and-standards.md#test-types-and-organization]

## Dev Notes

### Previous Story Insights

- CI/CD workflows from Story 1.7 already run formatting, lint, and type checks; any new scripts (e.g., schema validators) must integrate with this structure and keep Python 3.9 compatibility to avoid pipeline regressions. [Source: docs/stories/1.7.setup-github-actions-ci-cd-pipeline.md#dev-notes]

### Data Models

- CompiledDataset records track compiler version, source data cards, and data quality summaries‚Äîschema documentation must ensure columns map cleanly back to these attributes for reproducibility. [Source: architecture/data-models.md#model-4-compileddataset-ÁºñËØëÊï∞ÊçÆ]
- DataPoint entries always include `source_label` and evidence; the compiled schema needs to retain label fidelity (üü¢/üü°/üî¥) and provide room for evidence references when relevant. [Source: architecture/data-models.md#model-2-datapoint-Êï∞ÊçÆ]

### API Specifications

- No specific external API requirements for schema file generation were identified in architecture docs. [Source: architecture/external-apis.md#api-1-claude-code-ÂÜÖÁΩÆ-ai-Ê®°ÂûãË∞ÉÁî®]

### Component Specifications

- Compiler Engine aggregates DataCards into DataFrames, generates schema-aware outputs, and produces quality summaries; documentation should outline how the engine maps card attributes to tabular columns and anticipates heterogeneous inputs. [Source: architecture/components.md#component-4-compiler-engine]

### File Locations

- Data cards live under `data_cards/` within each project, while compiled datasets default to `compiled/`; the schema guidance must clarify these paths so automation scripts can locate inputs/outputs deterministically. [Source: architecture/source-tree.md#source-tree]
- Gold-standard data cards already reside in `tests/validation/gold_standards/`, offering immediate samples for schema examples and conversion tests. [Source: tests/validation/gold_standards/README.md]

### Testing

- Continuous testing mandates cross-platform coverage; compatibility scripts should run through `pytest` (even if lightweight) so CI jobs can execute them uniformly. [Source: architecture/test-strategy-and-standards.md#continuous-testing]
- Testing structure follows AAA patterns under `tests/` with clear fixtures; provide fixtures or helpers that load sample data cards and assert parsed DataFrame columns for both pandas and readr flows. [Source: architecture/test-strategy-and-standards.md#test-types-and-organization]

### Technical Constraints

- Python code must target 3.9+ with pandas 2.2+ for DataFrame operations; R examples should assume modern `readr` usage to stay aligned with documented toolchain expectations. [Source: architecture/tech-stack.md#technology-stack-table]
- Three-color labeling is a critical invariant‚Äîschema fields and examples must preserve emoji encoding to maintain downstream Oracle analysis fidelity. [Source: architecture/high-level-architecture.md#pattern-3-three-color-source-labeling-system]

## Tasks / Subtasks

- [x] Draft `docs/compiled-data-schema.md` with column definitions, data types, and provenance notes (AC: 1, 2, 3, 4, 7)
  - [x] Describe required columns, expected value formats, and enumerations for labels and outcome types. (AC: 1, 3, 4)
  - [x] Document metadata lineage fields linking back to DataCards and extraction context. (AC: 2)
  - [x] Include guidance for missing data handling, unit normalization, and heterogenous effect measures. (AC: 7)
- [x] Prepare a compiled sample dataset from at least five existing data cards and store it under `examples/sample_meta_analysis/compiled/`. (AC: 5)
  - [x] Convert gold-standard and automated sample cards into the standardized schema to demonstrate mixed sources. (AC: 5)
- [x] Implement validation scripts confirming `pandas.read_csv` and `readr::read_csv` can ingest the schema without column loss or type issues. (AC: 6)
  - [x] Add automated tests under `tests/validation/` (or a designated integration suite) that exercise both Python and R compatibility checks. (AC: 6)
- [x] Ensure documentation cross-links to Data Card format and quality labeling rationale for maintainability. (AC: 4)
- [x] Update repo index or README references if new documents or examples need discoverability cues. (AC: 1, 5)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-22 | 0.1 | Initial draft outlining schema deliverables and architecture alignment for Story 2.1. | Bob (Scrum Master) |

## Dev Agent Record

*This section populated by the development agent during implementation.*

### Agent Model Used

- GPT-5 Codex (James)

### Debug Log References

- `pytest tests/validation/test_compiled_schema_io.py`
- `Rscript -e "install.packages('readr', repos='https://cloud.r-project.org')"` (completed with lifecycle warning, readr installed)
- `Rscript tests/validation/test_compiled_schema_readr.R`

### Completion Notes List

- Authored `docs/compiled-data-schema.md` detailing required/optional fields, outcome handling, and maintenance guidance.
- Generated `examples/sample_meta_analysis/compiled/sample_compiled_data.csv` (mirrored at `examples/sample_compiled_data.csv`) with five DataCards covering mixed gold-standard and automated sources.
- Added pandas and readr validation scripts under `tests/validation/` and referenced them from the schema documentation.
- Updated `README.md` to expose the new schema documentation, sample dataset, and validation utilities.
- Installed the R `readr` dependency and executed the compatibility script to confirm schema ingestion.

### File List

- docs/compiled-data-schema.md
- examples/sample_meta_analysis/compiled/sample_compiled_data.csv
- examples/sample_compiled_data.csv
- tests/validation/test_compiled_schema_io.py
- tests/validation/test_compiled_schema_readr.R
- README.md
## QA Results

*Pending QA review.*

### Review Date: 2025-10-22

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
- Documentation and dataset align with architecture guidelines; no structural issues observed in `docs/compiled-data-schema.md`.
- Sample dataset demonstrates required columns but label the log-scaled ratios in `examples/sample_meta_analysis/compiled/sample_compiled_data.csv` as `hazard_ratio` / `risk_ratio`, which conflicts with the schema guidance to tag transformed metrics (`log_hr`, `log_rr`) and could mislead analysts.

### Test Coverage Assessment
- `pytest tests/validation/test_compiled_schema_io.py`
- Attempted `Rscript tests/validation/test_compiled_schema_readr.R` (blocked: `Rscript` executable unavailable in current environment). Script review indicates it will validate column coverage and emoji handling once R is present.

### NFR Validation
- Security: PASS - documentation-only change.
- Performance: PASS - no runtime impact identified.
- Reliability: CONCERNS - sample dataset metric labels may cause downstream misinterpretation of log-transformed ratios.
- Maintainability: PASS - schema doc is thorough with clear change management steps.

### Risk Summary
- Medium risk: Mislabelled log-transformed ratios in the sample dataset can lead to incorrect analytical interpretation if users assume raw ratios; adjust metric values to `log_rr` / `log_hr` or revert effect size to the stated scale.

### Recommendations
- Immediate: Update `examples/sample_meta_analysis/compiled/sample_compiled_data.csv` (and mirror) to align `effect_size_metric` with the stored transformation names, or revise documentation to match the provided data files.
- Future: Add a note in `docs/compiled-data-schema.md` indicating dependency on the `Rscript` CLI for validation, so environments can provision it before running the QA script.

### Suggested Status
- Recommend `Changes Required` until the sample dataset metric labels are corrected.

---

### Review Date: 2025-10-22

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
- Confirmed the follow-up edits now label log-transformed ratios in both sample datasets as `log_hazard_ratio` / `log_risk_ratio`, aligning with schema guidance.
- No new issues detected in `docs/compiled-data-schema.md`; guidance remains consistent with the updated datasets.

### Test Coverage Assessment
- `pytest tests/validation/test_compiled_schema_io.py`
- Attempted `Rscript tests/validation/test_compiled_schema_readr.R` (still blocked: `Rscript` executable unavailable locally). Script logic unchanged and remains valid once R is provisioned.

### NFR Validation
- Security: PASS - documentation/data only.
- Performance: PASS - static assets unaffected.
- Reliability: PASS - sample data and schema now consistent, preventing misinterpretation of transformed metrics.
- Maintainability: PASS - assets remain well-structured; consider documenting the R dependency separately if helpful.

### Risk Summary
- Low residual risk: R compatibility script execution pending local R installation; this is an environmental prerequisite, not a code issue.

### Recommendations
- Immediate: None - dataset and documentation now consistent.
- Future: Document R CLI dependency in onboarding materials to smooth execution of the readr validation script.

### Suggested Status
- Recommend `Ready for Done`.


