# Case Study: Guiding a First-Time Meta-Analyst in K-12 Literacy

## Context
- **Beta tester:** Tester-03 (postdoctoral researcher in education with low technical comfort).  
- **Research question:** Evaluate literacy interventions for K-12 learners across randomized and quasi-experimental studies.  
- **Baseline workflow:** Manual PDF review with spreadsheets (≈110 minutes per paper).  
- **Prompt versions:** Microscope v1.1.0, Compiler v1.1.0, Oracle v1.1.0.

## Workflow
1. Followed Microscope v1.1.0 with the added plain-language guidance from Story 3.6 to interpret statistical prompts.  
2. Logged 12 papers (3 gold standards) using the Story 3.5 qualitative feedback survey to capture usability hurdles.  
3. Applied Compiler v1.1.0 validation checks to catch missing covariate labels before data export.  
4. Generated a small-sample random-effects model in Oracle v1.1.0, documenting the decision to keep exploratory status until further verification.

## Outcomes
- **Papers processed:** 12 total (met the onboarding goal for new analysts).  
- **Gold standard accuracy:** 3/3 papers achieved ≥90% field-level agreement (100% pass).  
- **Time savings:** Average extraction dropped from 110 → 48.9 minutes per paper (56% reduction) by week six.  
- **Confidence shift:** Comfort with MAestro outputs rose from 2.4 → 3.6 / 5, despite limited coding background.  
- **User testimonial:** “I’m not a statistics person, so there was a learning curve… but once I got it, MAestro definitely saved time compared to copying numbers into Excel.” — Tester-03.

## Lessons
- Plain-language elaborations and glossary inserts from Story 3.6 removed the steepest terminology barriers.  
- Compiler validation summaries gave early warning when quasi-experimental studies required additional moderator fields.  
- Recommended next iteration: bundle a “first three papers” coaching checklist into onboarding so non-technical teams close the confidence gap faster.
